# ğŸ§  KovalskyAI â€” Your Local AI Assistant

KovalskyAI is a **fully self-hosted**, blazing-fast AI chat interface built for privacy, speed, and simplicity.  
No external APIs. No cloud. No monthly fees.  
Just you and your model â€” on your own terms.

---

## âœ¨ Features

- âš¡ **Real-time streaming responses** (like ChatGPT)
- ğŸ¨ **Minimalist, dark-themed interface** with elegant animations (GSAP-powered)
- ğŸ§± **Self-hosted** backend â€” works offline, no dependencies on OpenAI or others
- ğŸ–¥ï¸ **Cross-platform**: Runs smoothly on both laptops and desktops
- ğŸ” **CPU & GPU** support â€” CUDA-accelerated via `koboldcpp`
- ğŸ’¬ **Memory-aware chat history** (retains previous conversation context)
- ğŸ›¡ï¸ **CORS enabled** â€” ready for frontend/backend interaction
- ğŸŒ Clean separation between backend (Flask + Kobold API) and frontend (HTML/CSS/JS)

---

## ğŸ“¦ How it works

- You run your **own model** locally with [KoboldCpp](https://github.com/LostRuins/koboldcpp)
- The backend (`app.py`) connects to it and forwards your input
- Streaming tokens are parsed and sent instantly to the frontend


---

## ğŸš§ Roadmap & Planned Features

Here's what we're working on next:

- ğŸ” **Search the web** from within chat (`Search on Web` button)
- ğŸ“ **Attach documents**: Feed `.txt`, `.pdf`, `.docx`, etc. to the model
- ğŸ§  **Context-aware uploads**: AI will read and reference attached files in conversation
- ğŸ’¾ **Save/load chat sessions** to local storage
- ğŸŒ **Multi-language interface support**
- ğŸ—‚ï¸ **Prompt templates** and personalities (creative, assistant, coder, etc.)

---

## ğŸ› ï¸ Installation
KovalskyAI uses:

Backend: Python Flask, requests, KoboldCpp-compatible API

Frontend: Pure HTML, CSS, JavaScript, with GSAP for animations

# Clone the project
```git clone https://github.com/maxadov/KovalskyAI.git```  
```cd KovalskyAI```

# Create virtual environment
```python -m venv venv```  
```source venv/bin/activate```    ```On Windows: venv\Scripts\activate```

# Install dependencies
```pip install -r requirements.txt```

# Start the server
```python app.py```

Make sure KoboldCpp is running at http://localhost:5001/v1/chat/completions.

Frontend is served at: `http://localhost:8000`


ğŸ¤ Contributing
Contributions are warmly welcome! ğŸ”§
If youâ€™d like to suggest a feature, fix a bug, or improve anything:

1. Create an issue with details

2. Fork the repo

3. Open a pull request when ready

Letâ€™s build something awesome together.  


`Made with â¤ï¸ and local compute.`
