# ğŸ§  KovalskyAI â€” Your Local AI Assistant

KovalskyAI is a **fully self-hosted**, blazing-fast AI chat interface built for privacy, speed, and simplicity.  
No external APIs. No cloud. No monthly fees.  
Just you and your model â€” on your own terms.

---

![Demo](https://i.imgur.com/HECHphb.gif)

## âœ¨ Features

- âš¡ **Real-time streaming responses** (like ChatGPT)  
- ğŸ¨ **Minimalist, dark-themed interface** with elegant animations (GSAP-powered)  
- ğŸ§± **Self-hosted** backend â€” works offline, no dependencies on OpenAI or others  
- ğŸ–¥ï¸ **Cross-platform**: Runs smoothly on both laptops and desktops  
- ğŸ” **CPU & GPU** support â€” CUDA-accelerated via `koboldcpp`  
- ğŸ’¬ **Memory-aware chat history** (retains previous conversation context)  
- ğŸ›¡ï¸ **CORS enabled** â€” ready for frontend/backend interaction  
- ğŸŒ Clean separation between backend (Flask + Kobold API) and frontend (HTML/CSS/JS)  

---

## ğŸ“¦ How it works

- You run your **own model** locally with [KoboldCpp](https://github.com/LostRuins/koboldcpp)  
- The backend (`app.py`) connects to it and forwards your input  
- Streaming tokens are parsed and sent instantly to the frontend  

---

## ğŸš§ Roadmap & Planned Features

Here's what we're working on next:

- ğŸ” **Search the web** from within chat (`Search on Web` button)  
- ğŸ“ **Attach documents**: Feed `.txt`, `.pdf`, `.docx`, etc. to the model  
- ğŸ§  **Context-aware uploads**: AI will read and reference attached files in conversation  
- ğŸ’¾ **Save/load chat sessions** to local storage  
- ğŸŒ **Multi-language interface support**  
- ğŸ—‚ï¸ **Prompt templates** and personalities (creative, assistant, coder, etc.)  

---

## ğŸ› ï¸ Installation

KovalskyAI uses:

- **Backend**: Python `Flask`, `requests`, and a KoboldCpp-compatible API  
- **Frontend**: Pure `HTML`, `CSS`, `JavaScript` + `GSAP` for animations

### âš™ï¸ Model & Backend Setup
1. ğŸ“¥ **Download KoboldCpp**
Download the latest `KoboldCpp` build from [Releases](https://github.com/LostRuins/koboldcpp/releases)
Use the version with `cu12` in the name if you have an `NVIDIA GPU with CUDA 12`.  

2. ğŸ¤– **Download the Model**
Get the model file `qwen1_5-7b-chat-q4_k_m.gguf` from Hugging Face.
Choose `q4_k_m` format for a good balance of performance and quality.
_Place the `.gguf` model file in the same folder as `koboldcpp_cu12.exe.`_  


3. ğŸš€ **Launch KoboldCpp**
Start the local API with the following command:
```bash
.\koboldcpp_cu12.exe ^
  --model .\qwen1_5-7b-chat-q4_k_m.gguf ^
  --usecublas ^
  --gpulayers 33 ^
  --threads 8 ^
  --port 5001
```
_Adjust `--gpulayers` and `--threads` based on your hardware._

### Clone the project

```bash
git clone https://github.com/maxadov/KovalskyAI.git
cd KovalskyAI
```

Create a virtual environment
```bash
python -m venv venv
```

Activate it
Linux/macOS:
```bash
source venv/bin/activate
```
Windows:
```bash
venv\Scripts\activate
```


Install dependencies
```bash
pip install -r requirements.txt
```

Start the server
```bash
python app.py
```
âš ï¸ **Make sure KoboldCpp is running at:**
http://localhost:5001/v1/chat/completions

âœ… **Frontend will be available at:**
http://localhost:8000  


# ğŸ¤Contributing
**Contributions are warmly welcome!**

ğŸ”§If youâ€™d like to suggest **a feature, fix a bug, or improve** anything:

1. Create an issue with details

2. Fork the repo

3. Open a pull request when ready  

`Made with â¤ï¸ and local compute.`

